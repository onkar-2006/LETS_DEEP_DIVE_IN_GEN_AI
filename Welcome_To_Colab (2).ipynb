{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#NOW start the codeing what we have learn yesterday the yestrday opic was the langvchain the the langchain is rthe frameword mostly used to build the genai application\n",
        "#for building the genai application the most useful thing is the step to build the gen-ai think\n",
        "#step one is collext the data the data is scan mbe found in the various format like the data iseither taken from the doc , or the textt file or the xml fikeor from the json format for tothere any so in that case the main thing is u have to used the data loader faeture to load any typs of the data"
      ],
      "metadata": {
        "id": "PvxoZr7TzLMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step-1 load the data\n",
        "from langchain_community.document_loaders import text_loader\n",
        "text_data=text_loader()\n",
        "text=text_loader.load()\n",
        "text"
      ],
      "metadata": {
        "id": "vL2XOLllz8Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchian_community.document_loaders import pyPDFloader\n",
        "pdf_data=pyPDFloader()\n",
        "pdf=pyPDFloader.load()\n",
        "pdf"
      ],
      "metadata": {
        "id": "T-wfi9QY1eZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4#the bs4 is the called as the buityful full soop library\n",
        "from langchain_community.document_loaders import WebBasedLoader\n",
        "loader=WebBasedLoader(web_paths=\"\")\n",
        "bs_kwargs=dict(parse_only=bs4.soupStarainer(\n",
        "    class=(\"post-Header\" , \"post-Container\" , \"post-Subtitle\")\n",
        "))"
      ],
      "metadata": {
        "id": "eTQOsEPz1yB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import ArvixLoader\n",
        "docs=Arvixloader(query=\"research_paper\" , load_max_papers=2).load()\n",
        "load=loader.load()"
      ],
      "metadata": {
        "id": "tE2wbEo-22rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders. import WikipidiaLoader\n",
        "loader=WikipidiaLoader(query=\"name of the pagge\" , max_documents=2).load()\n",
        "load=loader.load()\n",
        "load"
      ],
      "metadata": {
        "id": "xURGitfh2PRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-splitter\n"
      ],
      "metadata": {
        "id": "PQ9qotn54Fqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100 )\n",
        "final_documents=text_splitter.split_documens(docs)\n",
        "final_documents"
      ],
      "metadata": {
        "id": "HGlkiwQk4KnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.documents_loaders import TextLoader\n",
        "text_data=TextLoader(\"sppech.txt\")\n",
        "lod=text_loader.load()\n",
        "lod\n",
        "\n"
      ],
      "metadata": {
        "id": "CNfXRNNN43qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speech=''\n",
        "with open(\"speech.txt\") as f:\n",
        "  speech=f.read()\n",
        "sppech\n",
        "\n"
      ],
      "metadata": {
        "id": "E0Yz_ILn5Znk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "text=text_splitter.create_documents([\"speech\"])\n",
        "text"
      ],
      "metadata": {
        "id": "K40Z4dRm5f66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "anathertype of the text_splitting tequenic"
      ],
      "metadata": {
        "id": "FfnY4K1k6Tkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "text_splitter=CharacterTextSplitter(separetor=\"\\n\\n\" , chunk_size=500 , chun_overlap=230)\n",
        "text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "nsURPCGF6BHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speech=''\n",
        "with open(\"speech.txt\") as f:\n",
        "  speech=f.read()\n",
        "\n",
        "  text_splitter=CharacterTextSplitter(chunk_size=500 , chunk_overlap=100)\n",
        "  text=text_splitter.create_documents([\"sppech\"])\n",
        "  text"
      ],
      "metadata": {
        "id": "aYuHtNhk7DAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now after load the data and also after the spliting the documnet or the texrt page or the html text the next step is to do the word embedding\n",
        "the word embedding is the tequenic used to convert the  the word into the vector atullayy we knowtthat the the machine do not lknow our language means the owrd to convertthe word to numenricss column is called the word embediing\n",
        "the word embedding in the genretive Al is done by three tequenic\n",
        "1-openAi\n",
        "2-olama\n",
        "3-huggingface models\n",
        "\n"
      ],
      "metadata": {
        "id": "hdixjJfIAB4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step to used the olama the\n",
        "step 1 -go to the olaman.com website form  that download the any model which u waant there are the lagre model are present\n",
        "step-2 :\n",
        "after lownlaod the exe file the enext step is tto install that model on ur own pc\n",
        "to install that file  on ur pc run the foolwing parameter suppose i used the gemini 3.0 model\n",
        "olama run gamma :2b\n",
        "\n",
        "download the langchain_community\n",
        "afterthe downoad the langchain_community the next step is coding\n"
      ],
      "metadata": {
        "id": "h6bj6LARAwNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "embedding=OllamaEmbeddings(model=\"Gamma-2\")\n"
      ],
      "metadata": {
        "id": "Z_D5vld4CuU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1X6vbdo_DQuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.embed_documents(\n",
        "    \"write any sample text w#uhich u want to test\"\n",
        ")"
      ],
      "metadata": {
        "id": "k591DQFCDgAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_query(\"what is the ftirst aname of the country will given by india\")"
      ],
      "metadata": {
        "id": "_2Le9nCjD_lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#u can also used the huggiface model it is the free models"
      ],
      "metadata": {
        "id": "I1m2JhdvEG0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOxmHtw-FaEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ],
      "metadata": {
        "id": "qFVZXOx0E87a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_token\"]=os.getenv(\"HF_TOKEN\")\n",
        "#for used the hugging face model the first step is to download the sentense_transformers ad also the langchain_huggingface"
      ],
      "metadata": {
        "id": "0WhDKWKEFcob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingfaceEmbeddings\n",
        "embeddings=HugginfaceEmbeddings(model_name=\"model_name which u wnat to give\")\n",
        "text=\"this is the sample text\"\n",
        "query_results=ebeddings.embed_query(text)"
      ],
      "metadata": {
        "id": "TNcWi32LHVwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_results"
      ],
      "metadata": {
        "id": "rdC_pcaYHe7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now after the spliitng the word the document into the words and after that the next step is onvert tht text into the vector we alredy convert the text into the vecor the next step is used the vectorbsee database to perform the next applicatio"
      ],
      "metadata": {
        "id": "e2vn58M9Hmhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "jOcjL_LDIKEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=TextLoader()\n",
        "doc=loader.load()\n",
        "text_splillter=CharacterTextSplitter(chunk_size=500 , chunk_overlap=50)\n",
        "docs=text_splitter.split_documents(docs)\n",
        "embeddings=OllamaEmbeddings()\n",
        "db=FAISS(docs , embeddings)"
      ],
      "metadata": {
        "id": "mkA-bv5XItQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"what is the capitl of the india\"\n",
        "db.similarity_search(query)"
      ],
      "metadata": {
        "id": "NTKwqKpTW-Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriver=db.as_retriver()\n",
        "retriver.invoke(query)"
      ],
      "metadata": {
        "id": "NYFHp8ZdXJX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import chroma\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_embeddings import OllamaEmbeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "gGQFSiDDYiyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=TextLoader(\"sppech.txt\")\n",
        "data=loader.load()"
      ],
      "metadata": {
        "id": "5pTXoqKdZSP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "split_data=RecursiveCharacterTextSplitter(chunk_size=1000 , chunk_overlap=100)\n",
        "sp=split_data.split_documents(docs)\n",
        "sp"
      ],
      "metadata": {
        "id": "-5vesQL9ZfTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding=OllamaEmbeddings()\n",
        "vectordb=chroma.from_documents(documents=sp , embedding=embedding)\n",
        "vectordb"
      ],
      "metadata": {
        "id": "JAYL-fO-aAHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query\n",
        "vectordb.similarity_search(query)\n",
        "doc[0].page_content"
      ],
      "metadata": {
        "id": "3qw38Xooabmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSYRStxra0PC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}